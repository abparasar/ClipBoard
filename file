from transformers import DataCollatorForSeq2Seq
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch

# === DataLoader for full dataset ===
collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="pt")
dataloader = DataLoader(tokenized_dataset["train"], batch_size=1, collate_fn=collator)

# === Set model to train mode ===
model.train()

# === Optimizer (basic AdamW) ===
from transformers import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5)

# === Loop through batches ===
for batch in tqdm(dataloader, desc="Training"):
    batch = {k: v.to(device) for k, v in batch.items()}
    
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()

    optimizer.step()
    optimizer.zero_grad()

    print(f"Loss: {loss.item():.4f}")