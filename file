from datasets import load_dataset, DatasetDict
from transformers import BartTokenizer
import torch

# Load the tokenizer (replace with local path if you have a local tokenizer)
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large")

# Set your max lengths
max_input_length = 1024
max_target_length = 128

# Load the dataset from local .jsonl files
data_files = {
    "train": "dialogsum/dialogsum.train.jsonl",
    "validation": "dialogsum/dialogsum.dev.jsonl",
    "test": "dialogsum/dialogsum.test.jsonl"
}

raw_datasets = load_dataset("json", data_files=data_files)

# Preprocessing function
def preprocess_function(examples):
    inputs = examples["dialogue"]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Tokenize targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize the dataset
tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

# Optional: move batches to CUDA during training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")