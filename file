from transformers import DataCollatorForSeq2Seq, AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch
import numpy as np
import nltk
import evaluate

nltk.download("punkt")
rouge = evaluate.load("rouge")

# === ROUGE-based compute_metrics function ===
def compute_metrics(preds, labels):
    # Format predictions and labels for ROUGE
    preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in labels]

    result = rouge.compute(predictions=preds, references=labels, use_stemmer=True)
    result = {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}

    # Add average generated length
    gen_lens = [len(pred.split()) for pred in preds]
    result["gen_len"] = round(np.mean(gen_lens), 2)

    return result

# === Loaders ===
collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="pt")
train_loader = DataLoader(tokenized_dataset["train"], batch_size=1, collate_fn=collator)
val_loader = DataLoader(tokenized_dataset["validation"], batch_size=1, collate_fn=collator)

# === Training Setup ===
model.train()
optimizer = AdamW(model.parameters(), lr=5e-5)
num_epochs = 5

# === Evaluation function ===
def evaluate(model, val_loader):
    model.eval()
    preds = []
    labels = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating", leave=False):
            batch = {k: v.to(device) for k, v in batch.items()}

            generated_tokens = model.generate(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=64,
                num_beams=4,
            )

            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
            # Fix -100 in labels for decoding
            label_ids = batch["labels"].clone()
            label_ids[label_ids == -100] = tokenizer.pad_token_id
            decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

            preds.extend(decoded_preds)
            labels.extend(decoded_labels)

    model.train()
    return compute_metrics(preds, labels)

# === Training Loop ===
for epoch in range(num_epochs):
    print(f"\nEpoch {epoch + 1}/{num_epochs}")
    
    total_loss = 0.0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}", leave=False)

    for step, batch in enumerate(progress_bar):
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(**batch)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        progress_bar.set_postfix({"loss": loss.item()})

    avg_loss = total_loss / len(train_loader)
    print(f">>> Epoch {epoch + 1} Average Train Loss: {avg_loss:.4f}")

    # === Evaluation ===
    metrics = evaluate(model, val_loader)
    print(f">>> Epoch {epoch + 1} Validation Metrics: {metrics}")