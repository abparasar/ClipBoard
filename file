import os
import torch
from datasets import load_dataset
from transformers import (
    BartTokenizer,
    BartForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)
from peft import get_peft_model, LoraConfig, TaskType

# ==== CONFIGURATION ====
LOCAL_MODEL_PATH = "./local_bart_large_cnn"  # Path to local BART-large-CNN
DIALOGSUM_DIR = "./dialogsum"  # Directory with DialogSum files

TRAIN_FILE = os.path.join(DIALOGSUM_DIR, "dialogsum.train.jsonl")
VAL_FILE = os.path.join(DIALOGSUM_DIR, "dialogsum.dev.jsonl")
TEST_FILE = os.path.join(DIALOGSUM_DIR, "dialogsum.test.jsonl")

OUTPUT_DIR = "./bart_lora_dialogsum_output"
MAX_INPUT_LEN = 512
MAX_TARGET_LEN = 128
BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 3e-4

# ==== DEVICE CHECK ====
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ==== LOAD TOKENIZER AND MODEL ====
tokenizer = BartTokenizer.from_pretrained(LOCAL_MODEL_PATH)
model = BartForConditionalGeneration.from_pretrained(LOCAL_MODEL_PATH).to(device)

# ==== APPLY LORA ====
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ==== LOAD DATA ====
data_files = {
    "train": TRAIN_FILE,
    "validation": VAL_FILE,
    "test": TEST_FILE
}
raw_datasets = load_dataset("json", data_files=data_files)

# ==== PREPROCESS ====
def preprocess(example, is_test=False):
    input_text = "dialogue: " + example["dialogue"]
    target_text = example["summary1"] if is_test else example["summary"]
    model_inputs = tokenizer(
        input_text,
        max_length=MAX_INPUT_LEN,
        truncation=True,
        padding="max_length"
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            target_text,
            max_length=MAX_TARGET_LEN,
            truncation=True,
            padding="max_length"
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Map datasets
tokenized_datasets = {
    "train": raw_datasets["train"].map(preprocess, batched=True, remove_columns=raw_datasets["train"].column_names),
    "validation": raw_datasets["validation"].map(preprocess, batched=True, remove_columns=raw_datasets["validation"].column_names),
    "test": raw_datasets["test"].map(lambda x: preprocess(x, is_test=True), batched=True, remove_columns=raw_datasets["test"].column_names),
}

# ==== TRAINING ARGS ====
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="steps",
    logging_steps=100,
    save_total_limit=2,
    fp16=True,  # Use mixed precision if GPU supports it
    report_to="none"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# ==== TRAINER ====
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

# ==== SAVE LORA ADAPTER ====
model.save_pretrained(os.path.join(OUTPUT_DIR, "lora_adapter"))
tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "lora_adapter"))