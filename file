Word embedding turns tokens into number vectors that capture meaning, so the model can understand and compare words.