from transformers import BartForConditionalGeneration
from transformers.utils import get_balanced_memory, infer_auto_device_map
from accelerate import init_empty_weights

# Step 1: Load the model skeleton without allocating memory
with init_empty_weights():
    model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

# Step 2: Set max memory for GPUs 1, 2, and 3 ONLY
max_memory = {
    1: "12GiB",
    2: "12GiB",
    3: "12GiB"
}

# Step 3: Identify layers that shouldn't be split
no_split_classes = ["BartEncoderLayer", "BartDecoderLayer"]

# Step 4: Get the device map
device_map = infer_auto_device_map(
    model,
    max_memory=max_memory,
    no_split_module_classes=no_split_classes
)

# Step 5: Load the model with that map
model = BartForConditionalGeneration.from_pretrained(
    "facebook/bart-large-cnn",
    device_map=device_map,
    low_cpu_mem_usage=True
)