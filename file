from transformers import DataCollatorForSeq2Seq, AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch

# === DataLoader ===
collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="pt")
dataloader = DataLoader(tokenized_dataset["train"], batch_size=1, collate_fn=collator)

# === Training Setup ===
model.train()
optimizer = AdamW(model.parameters(), lr=5e-5)
num_epochs = 5

# === Epoch Loop ===
for epoch in range(num_epochs):
    print(f"\nEpoch {epoch + 1}/{num_epochs}")
    
    total_loss = 0.0
    for step, batch in enumerate(tqdm(dataloader, desc=f"Training Epoch {epoch + 1}")):
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(**batch)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        print(f"Step {step + 1} - Loss: {loss.item():.4f}")
    
    avg_loss = total_loss / len(dataloader)
    print(f">>> Epoch {epoch + 1} Average Loss: {avg_loss:.4f}")