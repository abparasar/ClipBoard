from transformers import BartTokenizer, BartForConditionalGeneration
import torch

# Load the tokenizer and model from local directory
model_path = "./model"
tokenizer = BartTokenizer.from_pretrained(model_path)
model = BartForConditionalGeneration.from_pretrained(model_path)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Input text to summarize
text = """
The DialogSum dataset is a large-scale dialogue summarization corpus, covering a wide range of real-life topics. It is 
collected and constructed by annotating dialogues with corresponding summaries for training and evaluation of dialogue 
summarization models.
"""

# Tokenize input
inputs = tokenizer([text], max_length=1024, truncation=True, return_tensors="pt").to(device)

# Generate summary
summary_ids = model.generate(
    inputs["input_ids"],
    max_length=150,
    min_length=40,
    length_penalty=2.0,
    num_beams=4,
    early_stopping=True
)

# Decode and print summary
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print("Summary:", summary)