•	Encoder (e.g., BERT): Trained to select tokens (e.g., start and end) within the context → typically results in short, span-based answers.
	•	Decoder (e.g., GPT): Trained to generate the next token conditioned on prior tokens → naturally produces longer, fluent sentences.