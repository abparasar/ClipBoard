import pandas as pd
from openpyxl import load_workbook

# Load workbook and select active sheet
file_path = "your_file.xlsx"
wb = load_workbook(file_path, data_only=True)
ws = wb.active  # Change to ws = wb["SheetName"] if needed

# Extract data
data = []
for row in ws.iter_rows(values_only=True):
    data.append(row)

# Convert to DataFrame and set headers
df = pd.DataFrame(data)
df.columns = df.iloc[0]  # Set first row as header
df = df[1:]  # Remove header row from data

# Select only the required columns
columns_to_load = ["Column1", "Column2", "Column3", "HyperlinkCol1", "HyperlinkCol2", "Column6", "Column7", "Column8", "Column9", "Column10"]
df = df[columns_to_load]

# Display result
print(df.head())
\documentclass{article}
\usepackage{array}

\begin{document}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        (1,1) & \multicolumn{4}{|c|}{(1,2) to (1,5)} & (1,6) & (1,7) & (1,8) & (1,9) & (1,10) \\ \hline
        (2,1) & (2,2) & (2,3) & (2,4) & (2,5) & (2,6) & (2,7) & (2,8) & (2,9) & (2,10) \\ \hline
        (3,1) & (3,2) & (3,3) & (3,4) & (3,5) & (3,6) & (3,7) & (3,8) & (3,9) & (3,10) \\ \hline
        (4,1) & (4,2) & (4,3) & (4,4) & (4,5) & (4,6) & (4,7) & (4,8) & (4,9) & (4,10) \\ \hline
        (5,1) & (5,2) & (5,3) & (5,4) & (5,5) & (5,6) & (5,7) & (5,8) & (5,9) & (5,10) \\ \hline
        (6,1) & (6,2) & (6,3) & (6,4) & (6,5) & (6,6) & (6,7) & (6,8) & (6,9) & (6,10) \\ \hline
        (7,1) & (7,2) & (7,3) & (7,4) & (7,5) & (7,6) & (7,7) & (7,8) & (7,9) & (7,10) \\ \hline
        (8,1) & (8,2) & (8,3) & (8,4) & (8,5) & (8,6) & (8,7) & (8,8) & (8,9) & (8,10) \\ \hline
        (9,1) & (9,2) & (9,3) & (9,4) & (9,5) & (9,6) & (9,7) & (9,8) & (9,9) & (9,10) \\ \hline
        (10,1) & (10,2) & (10,3) & (10,4) & (10,5) & (10,6) & (10,7) & (10,8) & (10,9) & (10,10) \\ \hline
    \end{tabular}
    \caption{10x10 Table with Merged Columns}
    \label{tab:table1}
\end{table}

\end{document}


Alright! Imagine you have a big book written in a language you don’t understand. If you want a computer to read and understand it, you need to break it into smaller pieces—like words or even smaller parts—so the computer can work with them.

This breaking-down process is called tokenization. It helps the computer turn a long sentence into tiny chunks (tokens) that it can understand and process. Without tokenization, the computer would see a sentence as one giant mess and wouldn’t know where one word ends and another begins!

For example, if you give the computer:
“I love ice cream”

Tokenization turns it into:
	1.	“I”
	2.	“love”
	3.	“ice”
	4.	“cream”

Now, the encoder (which helps the computer understand meaning) can work with these pieces more easily!

Great question! Computers don’t understand words like humans do—they only understand numbers.

Imagine you’re playing a video game, and your character has a special code instead of a name, like Player_1234. The game doesn’t “see” your name; it just works with numbers behind the scenes.

Similarly, if we give a computer the words “I love ice cream”, it won’t know what they mean. But if we turn each word into a number, like:
	•	“I” → 1
	•	“love” → 42
	•	“ice” → 85
	•	“cream” → 128

Now the computer can process the numbers and 
find patterns, similarities, and meanings between
 them. This is how encoders help computers 
understand language!
Great thinking! Why not just turn words into vectors directly? Well, tokenization makes the process more efficient and flexible. Here’s why:

1. Words Can Be Split into Smaller Parts (Subwords)
	•	Imagine the words “unhappiness” and “happiness.” If we only convert whole words to vectors, we need a separate vector for both.
	•	But if we tokenize them into [“un”, “happiness”], we can reuse “happiness” and only learn “un” separately. This helps when dealing with new words!

2. Limited Vocabulary Issue
	•	If we turn every whole word into a vector, we’d need a dictionary with millions of words. But tokenization lets us reuse common parts, so we don’t need a giant dictionary.

3. Better for Different Languages
	•	Some languages, like Chinese, don’t have spaces between words. Tokenization helps break them into meaningful parts before converting them into vectors.

4. More Efficient Computation
	•	Instead of creating a separate vector for every possible word, tokenization reduces the number of unique things the model has to learn, making training and processing much faster!

So, tokenization first breaks text into useful pieces, and then we turn them into vectors for the computer to understand. It’s like chopping vegetables before cooking—it makes everything easier!



Great thinking! Why not just turn words into vectors directly? Well, tokenization makes the process more efficient and flexible. Here’s why:

1. Words Can Be Split into Smaller Parts (Subwords)
	•	Imagine the words “unhappiness” and “happiness.” If we only convert whole words to vectors, we need a separate vector for both.
	•	But if we tokenize them into [“un”, “happiness”], we can reuse “happiness” and only learn “un” separately. This helps when dealing with new words!

2. Limited Vocabulary Issue
	•	If we turn every whole word into a vector, we’d need a dictionary with millions of words. But tokenization lets us reuse common parts, so we don’t need a giant dictionary.

3. Better for Different Languages
	•	Some languages, like Chinese, don’t have spaces between words. Tokenization helps break them into meaningful parts before converting them into vectors.

4. More Efficient Computation
	•	Instead of creating a separate vector for every possible word, tokenization reduces the number of unique things the model has to learn, making training and processing much faster!

So, tokenization first breaks text into useful pieces, and then we turn them into vectors for the computer to understand. It’s like chopping vegetables before cooking—it makes everything easier!



          ┌──────────────────┐  
          │   Input Text     │  
          └────────▲─────────┘  
                   │  
                   ▼  
          ┌──────────────────┐  
          │  Normalization   │  (Lowercasing, removing special characters)  
          └────────▲─────────┘  
                   │  
                   ▼  
          ┌──────────────────┐  
          │ Splitting Tokens │  (Word, subword, or character level)  
          └────────▲─────────┘  
                   │  
                   ▼  
          ┌──────────────────┐  
          │ Assign Token IDs │  (Map tokens to numbers using a vocabulary)  
          └────────▲─────────┘  
                   │  
                   ▼  
          ┌──────────────────┐  
          │ Output Tokens/IDs│  (Final tokenized text ready for processing)  
          └──────────────────┘  

This process makes text easier for computers to understand and process efficiently!


Major Difference Between Tokenization and Embedding

Tokenization is about breaking text into smaller units (tokens) and mapping them to unique numerical IDs. It prepares text for further processing.

Embedding is about converting these token IDs into dense numerical vectors that capture meaning and relationships between words.

Example:

Sentence: "I love ice cream"
	1.	Tokenization → Converts words into tokens and assigns IDs:
	•	"I" → 101
	•	"love" → 234
	•	"ice" → 567
	•	"cream" → 890
	2.	Embedding → Converts token IDs into meaningful numerical vectors:
	•	101 → [0.1, 0.3, -0.2, …]
	•	234 → [0.5, -0.1, 0.8, …]
	•	567 → [-0.3, 0.7, 0.2, …]
	•	890 → [0.6, -0.4, 0.9, …]

Key Difference:
	•	Tokenization = Breaking text into pieces & assigning IDs
	•	Embedding = Converting IDs into meaningful number representations




