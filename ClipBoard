This is a great question and touches on an increasingly important area: LLM-based evaluation versus metric-based evaluation (like BERTScore). Let‚Äôs break down what might be happening and how to improve the LLM judge accuracy on Pegasus summaries.

‚∏ª

‚ö†Ô∏è Why Your LLaMA 70B Judge Is Scoring Pegasus Lower Despite Similar BERTScores

1. Bias from Few-Shot Examples

If your few-shot examples given to LLaMA-70B are mostly from BART-Large, then:
	‚Ä¢	The judge implicitly learns what ‚Äúgood summary‚Äù means only in BART terms.
	‚Ä¢	So it may penalize stylistic or structural differences in Pegasus summaries, even when they are semantically accurate.

2. Style vs. Semantic Bias

BERTScore evaluates semantic similarity based on contextual embeddings, so two summaries can score similarly even if:
	‚Ä¢	BART uses extractive and direct phrasing.
	‚Ä¢	Pegasus uses abstractive, paraphrased content.

LLaMA, however, being language-sensitive, may penalize:
	‚Ä¢	Overly concise Pegasus outputs
	‚Ä¢	Abstract expressions
	‚Ä¢	Different sentence structures

3. Prompt Bias / Hallucination Sensitivity

LLaMA might be:
	‚Ä¢	More sensitive to minor hallucinations or paraphrasing in Pegasus summaries
	‚Ä¢	Interpreting Pegasus outputs as less aligned to source texts even if meaning is preserved

‚∏ª

‚úÖ How to Improve LLaMA 70B Judge Accuracy on Pegasus Summaries

1. Balance the Few-Shot Examples

Provide diverse and high-quality examples from both:
	‚Ä¢	BART-Large summaries
	‚Ä¢	Pegasus summaries
Especially if their semantic meaning is equivalent. Show LLaMA what a ‚Äúgood Pegasus summary‚Äù looks like too.

Example Prompt Setup:

You are a summarization quality judge. Score between 1 (poor) and 5 (excellent).

Here are examples:

TEXT: [source text]
SUMMARY: [Pegasus summary]
SCORE: 5

TEXT: [source text]
SUMMARY: [Pegasus summary with minor issues]
SCORE: 3

TEXT: [source text]
SUMMARY: [BART summary]
SCORE: 5

...

Now evaluate the following:
TEXT: {text}
SUMMARY: {model_summary}
SCORE:

2. Give Dual Reference Ground Truth

Include the ground truth and ask the judge to score based on fidelity and completeness compared to both input text and reference summary, not just the input.

3. Use Chain-of-Thought Prompting

Ask LLaMA to reason before scoring, e.g.:

‚ÄúEvaluate how well the summary captures the key points of the text. Comment on coverage, factual correctness, and conciseness. Then assign a score.‚Äù

This leads to more transparent and possibly fairer scoring.

4. Prompt Neutralization

Avoid words in the prompt that bias toward one model (e.g., ‚ÄúBART-generated summaries are excellent‚Ä¶‚Äù). Keep it neutral.

‚∏ª

üõ† Optional: Fine-Tune or Calibrate Your LLaMA Judge

If you‚Äôre repeatedly using it for evaluation:
	‚Ä¢	You can fine-tune LLaMA 70B on summarization rating data (e.g., OpenAI‚Äôs summarization feedback dataset).
	‚Ä¢	Or use smaller models fine-tuned for evaluation, like:
	‚Ä¢	gpt-4 (via API)
	‚Ä¢	openai/summarize_from_feedback fine-tuned models
	‚Ä¢	llm-blender/summary-quality-eval (on HF)

‚∏ª

üìä Extra Tip: Compare Pegasus and BART on Specific Axes

Use your LLaMA judge or scripted rubric to compare both models on:
	‚Ä¢	Coverage
	‚Ä¢	Factual correctness
	‚Ä¢	Conciseness
	‚Ä¢	Coherence

Often, one model is better in abstraction (Pegasus) and the other in extractive faithfulness (BART). LLaMA might overvalue one over the other unless explicitly balanced.

‚∏ª

Would you like help crafting an optimal prompt for this? I can also help design a calibration experiment if needed.