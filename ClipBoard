ğŸ”¥ Perfect â€” letâ€™s level up your notebook-friendly random search to run FAST on 4 GPUs â€” the right way.
Hereâ€™s exactly how you do it step by step, with real multi-GPU DDP using torchrun (the official PyTorch launcher for Trainer).

â¸»

âœ… Key truth

Real multi-GPU with ğŸ¤— Trainer means:
	â€¢	Each GPU = its own Python process.
	â€¢	Those processes share gradients via PyTorch DDP.
	â€¢	This cannot run inside one .ipynb cell because notebooks run in a single process.

ğŸ‘‰ So: your code must go in a .py file, then launch with:

torchrun --nproc_per_node=4 train.py


â¸»

âš¡ï¸ Hereâ€™s your clean multi-GPU train.py

ğŸ‘‡ Copyâ€“paste this as train.py:

# train.py

import os
import torch
import pandas as pd
from sklearn.model_selection import train_test_split, ParameterSampler
from datasets import Dataset
from transformers import (
    PegasusTokenizer,
    PegasusForConditionalGeneration,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments
)
import evaluate
import numpy as np

# âœ… Required for DDP
local_rank = int(os.environ.get("LOCAL_RANK", 0))
device = torch.device("cuda", local_rank) if torch.cuda.is_available() else torch.device("cpu")

print(f"Rank {local_rank} using {device}")

# âœ… Load dataset
df = pd.read_csv("/projectspace/zkfq4hl/data/titlesum.csv").head(1000)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-large")
model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-large").to(device)

# âœ… Tokenize
def preprocess(examples):
    inputs = examples["dialogue"]
    model_inputs = tokenizer(
        inputs, max_length=1024, truncation=True, padding="max_length"
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["one_liner"],
            max_length=1024, truncation=True, padding="max_length"
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_ds = Dataset.from_pandas(train_df).map(preprocess, batched=True)
val_ds = Dataset.from_pandas(val_df).map(preprocess, batched=True)

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if isinstance(predictions, tuple):
        predictions = predictions[0]
    predictions = np.where(predictions >= 0, predictions, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return {k: round(v, 4) for k, v in rouge.compute(predictions=decoded_preds, references=decoded_labels).items()}

# âœ… Random search
param_dist = {
    "learning_rate": [1e-5, 3e-5, 5e-5],
    "per_device_train_batch_size": [2, 4],
    "num_train_epochs": [2, 3],
    "weight_decay": [0.0, 0.01],
    "warmup_steps": [0, 500],
}

param_sampler = ParameterSampler(param_dist, n_iter=5, random_state=42)

best_metric = -1
best_params = None

for i, params in enumerate(param_sampler):
    run_name = f"run_{i+1}_lr{params['learning_rate']}_bs{params['per_device_train_batch_size']}_ep{params['num_train_epochs']}_wd{params['weight_decay']}_wu{params['warmup_steps']}"
    output_dir = f"./pegasus_random_runs/{run_name}"

    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=params["learning_rate"],
        per_device_train_batch_size=params["per_device_train_batch_size"],
        per_device_eval_batch_size=params["per_device_train_batch_size"],
        num_train_epochs=params["num_train_epochs"],
        weight_decay=params["weight_decay"],
        warmup_steps=params["warmup_steps"],
        predict_with_generate=True,
        generation_max_length=1024,
        fp16=True,
        save_total_limit=1,
        logging_dir=f"{output_dir}/logs",
        logging_steps=50,
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="eval_rougeL",
        greater_is_better=True
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    if local_rank == 0:
        print(f"\nğŸš€ Starting run {i+1}: {run_name}\n{'-'*60}")

    trainer.train()
    metrics = trainer.evaluate()
    rougeL = metrics.get("eval_rougeL", 0)

    if local_rank == 0:
        print(f"Run {run_name} â†’ ROUGE-L: {rougeL:.4f}")

    if rougeL > best_metric:
        best_metric = rougeL
        best_params = params

if local_rank == 0:
    print("\nâœ… Finished all runs.")
    print(f"Best ROUGE-L: {best_metric:.4f}")
    print(f"Best Params: {best_params}")


â¸»

âœ… How to launch it

torchrun --nproc_per_node=4 train.py

This spawns 4 parallel Python processes, each on its own GPU:
	â€¢	local_rank=0 â†’ GPU 0
	â€¢	local_rank=1 â†’ GPU 1
	â€¢	â€¦ and so on.

The ğŸ¤— Trainer automatically syncs gradients across them â€” no extra code needed!

â¸»

âœ… Things to remember

âœ… Per-device batch â†’ each GPU sees per_device_train_batch_size.
So total batch = per_device_train_batch_size Ã— num_GPUs.

âœ… Logs & checkpoints only come from rank 0 â€” so you donâ€™t get duplicate output.

âœ… If you want fp16 â†’ set fp16=True.

â¸»

âœ… How to run from notebook

You canâ€™t do this directly inside .ipynb.
But you can launch it from a notebook cell with !:

!torchrun --nproc_per_node=4 train.py


â¸»

âœ… This is 100% the standard best practice for real multi-GPU Pegasus fine-tuning.

When youâ€™re ready for:
	â€¢	Cluster/Slurm job
	â€¢	W&B sweep logging
	â€¢	Better reproducibility

Just say: â€œShow me that next!â€ ğŸš€


âœ… Good call â€” by default, ğŸ¤— Trainer saves the best checkpoint for each run, because you used:

load_best_model_at_end=True,
metric_for_best_model="eval_rougeL",
greater_is_better=True

But that only keeps the best checkpoint per run (e.g. checkpoint-xxx).
If you want to automatically copy the final best model to a named folder at the end of your entire random search, just add this ğŸ‘‡ after your loop:

â¸»

âœ… Add to your train.py

from transformers import AutoModelForSeq2SeqLM

if local_rank == 0:
    print("\nâœ… Finished all runs.")
    print(f"Best ROUGE-L: {best_metric:.4f}")
    print(f"Best Params: {best_params}")

    # ğŸ’¾ Save final best model under a clear name
    final_output_dir = "./pegasus_random_runs/best_model_final"

    # Re-train with best params (or just reload last checkpoint)
    best_run_name = f"best_run_lr{best_params['learning_rate']}_bs{best_params['per_device_train_batch_size']}_ep{best_params['num_train_epochs']}_wd{best_params['weight_decay']}_wu{best_params['warmup_steps']}"
    best_output_dir = f"./pegasus_random_runs/{best_run_name}"

    print(f"\nâœ… Saving final best model from: {best_output_dir}")

    # Reload best model from output_dir
    best_model = PegasusForConditionalGeneration.from_pretrained(best_output_dir)
    best_model.save_pretrained(final_output_dir)
    tokenizer.save_pretrained(final_output_dir)

    print(f"\nğŸš€ Best model saved to: {final_output_dir}")


â¸»

ğŸ”‘ What this does

âœ… Keeps the final best model in:

./pegasus_random_runs/best_model_final

âœ… Uses the best runâ€™s folder you trained in â†’ reloads it â†’ saves it cleanly.

âœ… You can then from_pretrained() from that folder in future runs:

model = PegasusForConditionalGeneration.from_pretrained("./pegasus_random_runs/best_model_final")


â¸»

âš¡ï¸ Thatâ€™s it!

This makes your random search fully self-contained:
	â€¢	Each trial â†’ best checkpoint saved.
	â€¢	Final best â†’ cleanly exported to one folder.

â¸»

If youâ€™d like, I can show you:
	â€¢	How to push this folder directly to ğŸ¤— Hub.
	â€¢	How to load and generate with your final model.
	â€¢	Or how to log all runs nicely with wandb.

Just say â€œShow me next!â€ ğŸš€