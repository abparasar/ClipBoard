Self-attention lets each word in a sentence focus on all other words—including itself—to understand context and meaning.
Residual connection adds the original input back to the output of a layer to help preserve information and improve training.