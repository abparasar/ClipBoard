Got it â€” letâ€™s break this down properly so you can measure the variability between the two testers for each metric.

â¸»

âœ… What you have
	â€¢	1 Excel file
	â€¢	1 column with 500 rows of summaries (transcripts)
	â€¢	2 testers rate each row on:
	â€¢	Metric 1: 1â€“5 scale (ordinal)
	â€¢	Metric 2: 0 or 1 (binary)

â¸»

âœ… What you want
	â€¢	Measure how consistent the two testers are:
	â€¢	For Metric 1 â†’ Agreement on ordinal scores
	â€¢	For Metric 2 â†’ Agreement on binary labels

â¸»

âœ… How to check agreement

Hereâ€™s whatâ€™s standard:

Metric	Common agreement check
Metric 1 (1â€“5)	Cohenâ€™s Kappa, Weighted Kappa, or Intraclass Correlation Coefficient (ICC)
Metric 2 (0/1)	Cohenâ€™s Kappa, Simple percent agreement


â¸»

âœ… Why these
	â€¢	Cohenâ€™s Kappa â†’ Measures agreement beyond chance.
	â€¢	Weighted Kappa â†’ Better for ordinal scales since it penalizes bigger disagreements more.
	â€¢	ICC â†’ Good if you treat scores as continuous/ordinal data.

â¸»

âœ… How to do it in Excel + Python

Hereâ€™s a clear process:

â¸»

ğŸ“‚ 1ï¸âƒ£ Prepare your data

Your sheet should look like:

Summary	Tester1_Metric1	Tester2_Metric1	Tester1_Metric2	Tester2_Metric2
â€¦	â€¦	â€¦	â€¦	â€¦


â¸»

ğŸ§® 2ï¸âƒ£ Run agreement checks

ğŸ‘‰ For Metric 1 (1â€“5):

Option A: Weighted Cohenâ€™s Kappa

Python example:

import pandas as pd
from sklearn.metrics import cohen_kappa_score

df = pd.read_excel('your_file.xlsx')

kappa_metric1 = cohen_kappa_score(
    df['Tester1_Metric1'],
    df['Tester2_Metric1'],
    weights='quadratic'  # weighted kappa for ordinal
)

print(f"Weighted Cohen's Kappa for Metric 1: {kappa_metric1:.3f}")


â¸»

ğŸ‘‰ For Metric 2 (0/1):

Option B: Simple Cohenâ€™s Kappa

kappa_metric2 = cohen_kappa_score(
    df['Tester1_Metric2'],
    df['Tester2_Metric2']
)

print(f"Cohen's Kappa for Metric 2: {kappa_metric2:.3f}")


â¸»

ğŸ“Š 3ï¸âƒ£ Interpret

Kappa	Strength of agreement
â‰¤ 0	Poor
0.01â€“0.20	Slight
0.21â€“0.40	Fair
0.41â€“0.60	Moderate
0.61â€“0.80	Substantial
0.81â€“1.00	Almost perfect


â¸»

âœ… Extra: See disagreements

If you want to see where they disagree, do:

df['Disagree_Metric1'] = df['Tester1_Metric1'] != df['Tester2_Metric1']
df['Disagree_Metric2'] = df['Tester1_Metric2'] != df['Tester2_Metric2']

# Count
print(df['Disagree_Metric1'].value_counts())
print(df['Disagree_Metric2'].value_counts())


â¸»

ğŸ“Œ If you want to do it all in Excel

Youâ€™ll need to:
	â€¢	Calculate percent agreement manually
	â€¢	Use a confusion matrix for each metric
	â€¢	Or run a macro / plugin for Cohenâ€™s Kappa

â¸»

If youâ€™d like, I can make you a ready Python script or Excel sheet to calculate all of this. Want that?