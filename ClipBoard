\documentclass{article}
\usepackage{geometry}
\usepackage{booktabs}
\begin{document}

\begin{table}[h!]
\centering
\caption{Technical Feature Comparison: BART-Base vs BART-Large vs Pegasus}
\begin{tabular}{@{}llccc@{}}
\toprule
\textbf{Feature} & \textbf{Description} & \textbf{BART-Base} & \textbf{BART-Large} & \textbf{Pegasus} \\
\midrule
Model Type & Transformer Seq2Seq & Yes & Yes & Yes \\
Parameters & Total trainable params & 139M & 406M & 568M \\
Encoder Layers & Number of encoder layers & 6 & 12 & 16 \\
Decoder Layers & Number of decoder layers & 6 & 12 & 16 \\
Hidden Size & Dimensionality of hidden states & 768 & 1024 & 1024 \\
FFN Size & Feed-forward inner layer size & 3072 & 4096 & 4096 \\
Attention Heads & Number of attention heads & 12 & 16 & 16 \\
Max Position Embeddings & Max token positions & 1024 & 1024 & 1024 \\
Pretraining Objective & Training loss type & Denoising Autoencoder & Denoising Autoencoder & Gap Sentence Generation \\
Tokenization & Tokenizer used & BPE (fairseq) & BPE (fairseq) & SentencePiece \\
Training Data & Pretraining corpus & BookCorpus, CC-News, etc. & Same as base & Huge web corpus (C4, HugeNews, etc.) \\
Pretraining Tokens & Total tokens used in pretraining & ~160B & ~160B & ~500B \\
Training Framework & Original training library & fairseq & fairseq & TensorFlow / fairseq \\
Finetuning Task Types & Summarization, QA, etc. & Yes & Yes & Optimized for summarization \\
Language Coverage & Multilingual or monolingual & Monolingual (English) & Monolingual (English) & Monolingual (English) \\
Typical Use Case & Common downstream task & General NLP & General NLP & Summarization \\
\bottomrule
\end{tabular}
\end{table}

\end{document}