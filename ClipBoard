import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertForSequenceClassification

# Load your fine-tuned BERT model
model = BertForSequenceClassification.from_pretrained('path_to_your_model')
model.to('cuda')  # Ensure the model is on the GPU

# Prepare your tokenizer and dataset
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
test_dataset = ...  # Your test dataset

# Create a DataLoader for batching
batch_size = 32  # Adjust based on your GPU capacity
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Switch the model to evaluation mode
model.eval()

all_predictions = []

# Disable gradient calculation to reduce memory usage
with torch.no_grad():
    for batch in test_loader:
        # Move input tensors to GPU
        inputs = {key: value.to('cuda') for key, value in batch.items()}

        # Forward pass to get predictions
        outputs = model(**inputs)
        logits = outputs.logits

        # Convert logits to predicted class labels
        predictions = torch.argmax(logits, dim=-1)

        # Store predictions
        all_predictions.extend(predictions.cpu().numpy())

        # Clear memory
        torch.cuda.empty_cache()

# All predictions are now stored in all_predictions