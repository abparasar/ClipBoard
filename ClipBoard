Training a BERT model for intent classification involves several steps, including setting up the environment, preparing the dataset, fine-tuning the BERT model, saving the trained model, and then loading it to make predictions. Here is a step-by-step guide to accomplish this using Python and the Hugging Face `transformers` library.

### Step 1: Set Up the Environment

First, ensure you have the necessary libraries installed:

```bash
pip install transformers datasets torch
```

### Step 2: Prepare the Dataset

For this example, we'll use a simple dataset. Suppose we have a CSV file with two columns: `text` and `intent`.

```csv
text,intent
"Book a flight to New York","book_flight"
"What's the weather like tomorrow?","weather_query"
"Play some music","play_music"
...
```

### Step 3: Fine-tune BERT

We'll use a pre-trained BERT model and fine-tune it on our intent classification dataset.

```python
import pandas as pd
from datasets import load_dataset, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch

# Load the dataset
df = pd.read_csv('intent_data.csv')
dataset = Dataset.from_pandas(df)

# Preprocess the dataset
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Prepare the dataset for training
tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)
train_dataset = tokenized_datasets["train"]
eval_dataset = tokenized_datasets["test"]

# Load the model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['intent'].unique()))

# Define the training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# Train the model
trainer.train()

# Save the model
model.save_pretrained('./intent_model')
tokenizer.save_pretrained('./intent_model')
```

### Step 4: Load the Model and Make Predictions

After training and saving the model, we can load it and make predictions on new text inputs.

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Load the model and tokenizer
model = BertForSequenceClassification.from_pretrained('./intent_model')
tokenizer = BertTokenizer.from_pretrained('./intent_model')

# Function to predict intent
def predict_intent(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_id = torch.argmax(logits, dim=1).item()
    intent = model.config.id2label[predicted_class_id]
    return intent

# Test the prediction function
texts = ["Book a flight to London", "Tell me a joke", "Set an alarm for 7 AM"]
for text in texts:
    print(f"Text: {text} - Predicted Intent: {predict_intent(text)}")
```

This script will train a BERT model on the provided dataset, save the model, and load it for making predictions. Remember to replace `intent_data.csv` with the path to your actual dataset.