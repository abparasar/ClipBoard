import os
import logging
import warnings
import pandas as pd
import torch
from transformers import (
    BartForConditionalGeneration, BartTokenizer,
    Trainer, TrainingArguments
)
from datasets import load_dataset
from evaluate import load as load_metric
import bert_score
from sklearn.model_selection import ParameterSampler

# ==== SILENCE all verbose logs ====
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.ERROR)
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

import transformers
import datasets
transformers.logging.set_verbosity_error()
datasets.logging.set_verbosity_error()

# ==== Load Dataset ====
dataset = load_dataset("xsum")
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large")

# ==== Metrics ====
rouge = load_metric("rouge")

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    rouge_result = rouge.compute(predictions=preds, references=labels, use_stemmer=True)
    bertscore_result = bert_score.score(preds, labels, lang="en", verbose=False)
    avg_bertscore_f1 = bertscore_result[2].mean().item()

    return {
        "rouge1": rouge_result["rouge1"].mid.fmeasure,
        "rouge2": rouge_result["rouge2"].mid.fmeasure,
        "rougeL": rouge_result["rougeL"].mid.fmeasure,
        "rougeLsum": rouge_result["rougeLsum"].mid.fmeasure,
        "bertscore_f1": avg_bertscore_f1
    }

# ==== Dynamic Preprocessing Function ====
def make_preprocessing_fn(max_source_length, max_target_length=128):
    def preprocess(example):
        inputs = tokenizer(
            example['document'], truncation=True, padding="max_length",
            max_length=max_source_length
        )
        targets = tokenizer(
            example['summary'], truncation=True, padding="max_length",
            max_length=max_target_length
        )
        inputs['labels'] = targets['input_ids']
        return inputs
    return preprocess

# ==== Hyperparameter Space ====
param_grid = {
    'learning_rate': [1e-5, 3e-5, 5e-5],
    'per_device_train_batch_size': [4, 8],
    'gradient_accumulation_steps': [1, 2],
    'num_train_epochs': [2, 3],
    'weight_decay': [0.0, 0.01],
    'warmup_steps': [0, 250, 500],
    'lr_scheduler_type': ['linear', 'cosine'],
    'max_grad_norm': [0.8, 1.0],
    'label_smoothing_factor': [0.0, 0.1],
    'fp16': [True],
    'evaluation_strategy': ['epoch'],
    'eval_steps': [500],
    'save_total_limit': [1],
    'max_source_length': [512, 1024]
}

n_trials = 20
random_state = 42

param_combinations = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=random_state))

# ==== Result CSV Setup ====
csv_file = "results.csv"
columns = list(param_grid.keys()) + ["train_batch_size", "rouge1", "rouge2", "rougeL", "rougeLsum", "bertscore_f1"]
pd.DataFrame(columns=columns).to_csv(csv_file, index=False)

# ==== Tuning Loop ====
for i, params in enumerate(param_combinations):
    print(f"\nðŸš€ Trial {i+1}/{n_trials} | Params: {params}")

    # Preprocess with selected max_source_length
    preprocess_fn = make_preprocessing_fn(params['max_source_length'])

    train_subset = dataset["train"].select(range(10_000)).map(preprocess_fn, batched=True)
    eval_subset = dataset["validation"].select(range(500)).map(preprocess_fn, batched=True)

    model = BartForConditionalGeneration.from_pretrained("facebook/bart-large")

    # Extract and clean TrainingArguments
    eval_strategy = params.get('evaluation_strategy', 'epoch')
    eval_steps = params.get('eval_steps', None)

    args = TrainingArguments(
        output_dir=f"./runs/run_{i}",
        overwrite_output_dir=True,
        evaluation_strategy=eval_strategy,
        eval_steps=eval_steps if eval_strategy == 'steps' else None,
        logging_strategy="no",
        save_strategy="no",
        report_to="none",
        disable_tqdm=True,
        **{k: v for k, v in params.items() if k not in ['evaluation_strategy', 'eval_steps', 'max_source_length']}
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_subset,
        eval_dataset=eval_subset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    # ==== Train and Evaluate ====
    trainer.train()
    metrics = trainer.evaluate()

    # ==== Record Results ====
    record = {**params}
    record['train_batch_size'] = (
        params['per_device_train_batch_size'] * params['gradient_accumulation_steps']
    )
    for k in ["rouge1", "rouge2", "rougeL", "rougeLsum", "bertscore_f1"]:
        record[k] = metrics.get(k, None)

    pd.DataFrame([record]).to_csv(csv_file, mode='a', header=False, index=False)

    del trainer, model
    torch.cuda.empty_cache()