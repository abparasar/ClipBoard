Perfect — using ParameterSampler is a clean way to do random search with scikit-learn!
Let’s wire that into your BART-Large fine-tuning loop.

Here’s how you’d do it step-by-step:

⸻

✅ 1️⃣ Install scikit-learn

pip install scikit-learn


⸻

✅ 2️⃣ Define the Parameter Space

from sklearn.model_selection import ParameterSampler
import numpy as np

param_grid = {
    "learning_rate": np.linspace(1e-5, 5e-5, 1000),
    "weight_decay": np.linspace(0.0, 0.3, 1000),
    "per_device_train_batch_size": [2, 4, 8],
    "num_train_epochs": [2, 3, 4, 5],
    "lr_scheduler_type": ["linear", "cosine", "polynomial"],
}


⸻

✅ 3️⃣ Generate Random Samples

N_TRIALS = 5  # Or however many configs you want
random_state = 42

sampler = ParameterSampler(param_grid, n_iter=N_TRIALS, random_state=random_state)


⸻

✅ 4️⃣ Training Arguments Template

from transformers import Seq2SeqTrainingArguments

def get_training_arguments(params, output_dir):
    return Seq2SeqTrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",
        learning_rate=params["learning_rate"],
        weight_decay=params["weight_decay"],
        per_device_train_batch_size=params["per_device_train_batch_size"],
        per_device_eval_batch_size=params["per_device_train_batch_size"],
        num_train_epochs=params["num_train_epochs"],
        predict_with_generate=True,
        fp16=True,
        lr_scheduler_type=params["lr_scheduler_type"],
        seed=42,
        save_total_limit=1,
    )


⸻

✅ 5️⃣ Use with Your Trainer

from transformers import BartForConditionalGeneration, BartTokenizerFast, Seq2SeqTrainer
from datasets import load_dataset
from transformers import DataCollatorForSeq2Seq

model = BartForConditionalGeneration.from_pretrained("facebook/bart-large")
tokenizer = BartTokenizerFast.from_pretrained("facebook/bart-large")

dataset = load_dataset("cnn_dailymail", "3.0.0")

def preprocess_function(examples):
    inputs = examples["article"]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["highlights"], max_length=256, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized = dataset.map(preprocess_function, batched=True)
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

train_data = tokenized["train"].shuffle(seed=42).select(range(5000))
eval_data = tokenized["validation"].shuffle(seed=42).select(range(500))

best_metric = None
best_params = None

for i, params in enumerate(sampler):
    print(f"Trial {i+1} with: {params}")
    args = get_training_arguments(params, output_dir=f"./results/param_sampler_trial_{i+1}")

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=train_data,
        eval_dataset=eval_data,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    trainer.train()
    eval_metrics = trainer.evaluate()
    rougeL = eval_metrics.get("eval_rougeL", 0)
    print(f"Trial {i+1} | ROUGE-L: {rougeL:.4f}")

    if best_metric is None or rougeL > best_metric:
        best_metric = rougeL
        best_params = params

print(f"\n✅ Best Params: {best_params}\n✅ Best ROUGE-L: {best_metric:.4f}")


⸻

✅ Key Advantages
	•	ParameterSampler does uniform random sampling efficiently.
	•	random_state=42 gives reproducibility.
	•	You can expand param_grid to include more options or finer grids.
	•	Evolutionary tweaks: Take best_params and add new grids around them for next runs.

⸻

⚡ Ready to run!

If you want, I can generate an evolution strategy version that keeps mutating the best_params from ParameterSampler as well.

Just say: “Yes, show me the evolutionary loop!” and I’ll write it!