from sklearn.model_selection import StratifiedKFold
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the dataset (assuming df is already defined)
def tokenize_function(examples):
    return tokenizer(examples['utterance'], padding='max_length', truncation=True)

# Prepare the dataset
inputs = tokenizer(df['utterance'].tolist(), padding=True, truncation=True, return_tensors="pt")
labels = torch.tensor(df['intent'].tolist())

# Create a dataset from the tokenized inputs
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, inputs, labels):
        self.inputs = inputs
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.inputs['input_ids'][idx],
            'attention_mask': self.inputs['attention_mask'][idx],
            'labels': self.labels[idx],
        }

dataset = CustomDataset(inputs, labels)

# Define hyperparameter configurations (example)
hyperparameter_configs = [
    {'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'learning_rate': 5e-5},
    {'num_train_epochs': 5, 'per_device_train_batch_size': 4, 'learning_rate': 3e-5},
]

n_splits = 5
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Loop over each fold
for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(dataset)), dataset.labels.numpy())):
    print(f"Fold {fold + 1}/{n_splits}")

    train_fold_dataset = torch.utils.data.Subset(dataset, train_idx)
    val_fold_dataset = torch.utils.data.Subset(dataset, val_idx)

    # Loop through hyperparameter configurations
    for config in hyperparameter_configs:
        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(labels.numpy())))

        training_args = TrainingArguments(
            output_dir=f'./results_fold_{fold}_lr_{config["learning_rate"]}',
            num_train_epochs=config['num_train_epochs'],
            per_device_train_batch_size=config['per_device_train_batch_size'],
            per_device_eval_batch_size=16,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            logging_dir=f'./logs_fold_{fold}_lr_{config["learning_rate"]}',
            load_best_model_at_end=True,
            metric_for_best_model="accuracy",
        )

        # Initialize the Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_fold_dataset,
            eval_dataset=val_fold_dataset,
            compute_metrics=compute_metrics,
        )

        # Train and evaluate for the current fold
        trainer.train()

        # Save the best model for this fold and hyperparameter configuration
        trainer.save_model(f'./model_fold_{fold}_lr_{config["learning_rate"]}')

        # Optionally, you can evaluate and log results for each hyperparameter set