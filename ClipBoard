for i, params in enumerate(param_sampler):
    run_name = f"run_{i+1}_lr{params['learning_rate']}_bs{params['per_device_train_batch_size']}_ep{params['num_train_epochs']}_wd{params['weight_decay']}_wu{params['warmup_steps']}"
    output_dir = f"./pegasus_random_runs/{run_name}"

    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=params["learning_rate"],
        per_device_train_batch_size=params["per_device_train_batch_size"],
        per_device_eval_batch_size=params["per_device_train_batch_size"],
        num_train_epochs=params["num_train_epochs"],
        weight_decay=params["weight_decay"],
        warmup_steps=params["warmup_steps"],
        predict_with_generate=True,
        generation_max_length=1024,
        fp16=True,
        save_total_limit=1,
        logging_dir=f"{output_dir}/logs",
        logging_steps=50,
        logging_strategy="steps",
        disable_tqdm=False,   # <-- force tqdm for all ranks
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="eval_rougeL",
        greater_is_better=True
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # ðŸ‘‡ This print runs on every rank
    print(f"\nðŸš€ [RANK {local_rank}] Starting run {i+1}/{n_iter}")
    print(f"[RANK {local_rank}] Params: {params}")
    print(f"[RANK {local_rank}] Output dir: {output_dir}\n{'-'*60}")

    trainer.train()

    metrics = trainer.evaluate()
    rougeL = metrics.get("eval_rougeL", 0)

    # ðŸ‘‡ Each rank shows its eval too
    print(f"[RANK {local_rank}] Run {run_name} â†’ Eval Loss: {metrics['eval_loss']:.4f} | ROUGE-L: {rougeL:.4f}")

    if rougeL > best_metric:
        best_metric = rougeL
        best_params = params

# âœ… After all runs
print(f"\nâœ… [RANK {local_rank}] Finished all runs.")
print(f"[RANK {local_rank}] Best ROUGE-L: {best_metric:.4f}")
print(f"[RANK {local_rank}] Best Params: {best_params}")

# âœ… Only rank 0 saves final best model
if local_rank == 0:
    final_output_dir = "./pegasus_random_runs/best_model_final"
    best_run_name = f"best_run_lr{best_params['learning_rate']}_bs{best_params['per_device_train_batch_size']}_ep{best_params['num_train_epochs']}_wd{best_params['weight_decay']}_wu{best_params['warmup_steps']}"
    best_output_dir = f"./pegasus_random_runs/{best_run_name}"

    print(f"\nâœ… [RANK {local_rank}] Saving best model from: {best_output_dir}")
    best_model = PegasusForConditionalGeneration.from_pretrained(best_output_dir)
    best_model.save_pretrained(final_output_dir)
    tokenizer.save_pretrained(final_output_dir)
    print(f"\nðŸš€ [RANK {local_rank}] Best model saved to: {final_output_dir}")