import torch
from transformers import BertTokenizer, BertForSequenceClassification

# Load the fine-tuned BERT model and tokenizer
model_name = 'path_to_your_fine_tuned_model'
model = BertForSequenceClassification.from_pretrained(model_name)
tokenizer = BertTokenizer.from_pretrained(model_name)

# Ensure the model is in evaluation mode
model.eval()

# Example utterances to classify
utterances = ["Example utterance 1", "Another example", "More text to classify"]

# Tokenize the utterances
inputs = tokenizer(utterances, padding=True, truncation=True, return_tensors="pt")

# Move the model and inputs to the GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
inputs = {key: value.to(device) for key, value in inputs.items()}

# Perform inference to get logits
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# Apply softmax to get probabilities
probs = torch.softmax(logits, dim=-1)

# Set the threshold
threshold = 0.98

# Predict the class for each utterance if any probability exceeds the threshold
predicted_classes = []
for prob in probs:
    max_prob, predicted_class = torch.max(prob, dim=-1)
    if max_prob >= threshold:
        predicted_classes.append(predicted_class.item())  # Append the class index
    else:
        predicted_classes.append(None)  # No class meets the threshold

# Output results
for i, utterance in enumerate(utterances):
    if predicted_classes[i] is not None:
        print(f"'{utterance}' => Predicted Class: {predicted_classes[i]}")
    else:
        print(f"'{utterance}' => No class meets the threshold.")