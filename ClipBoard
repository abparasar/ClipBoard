import torch
import numpy as np

# Assuming 'words' is your list of words and 'flattened_embedding' is a tensor of shape (5676, 600)
# Assuming 'key' is your list of 3 different words

def find_closest_words(words, flattened_embedding, key, top_n=5):
    # Convert key list to its corresponding indices in words
    key_indices = [words.index(k) for k in key]
    
    # Normalize the embeddings to have unit length (important for cosine similarity)
    flattened_embedding = torch.nn.functional.normalize(flattened_embedding, p=2, dim=1)

    closest_words = {}
    
    # For each word in key, find the closest embeddings
    for i, k_idx in enumerate(key_indices):
        key_embedding = flattened_embedding[k_idx]  # Get the embedding of the key word
        
        # Calculate cosine similarities with all other embeddings
        similarities = torch.matmul(flattened_embedding, key_embedding)
        
        # Get the top N closest words (excluding the key word itself)
        _, top_indices = torch.topk(similarities, top_n + 1)  # +1 because the key word itself will have the highest similarity
        
        # Convert the indices to words
        closest_words[key[i]] = [words[idx] for idx in top_indices if idx != k_idx][:top_n]

    return closest_words


# Example usage
words = [...]  # Your list of 5676 words
flattened_embedding = torch.tensor([...])  # Your tensor of embeddings
key = ['word1', 'word2', 'word3']  # Your list of 3 key words

# Get the closest words to each word in the key list
closest_words = find_closest_words(words, flattened_embedding, key)

# Output the result
for k, v in closest_words.items():
    print(f"Closest words to '{k}': {v}")