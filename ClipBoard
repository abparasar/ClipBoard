import torch
import numpy as np

def find_closest_words(words, flattened_embedding, key, top_n=5):
    # Convert key list to its corresponding indices in words
    key_indices = [words.index(k) for k in key]
    
    # Normalize the embeddings to have unit length (important for cosine similarity)
    flattened_embedding = torch.nn.functional.normalize(flattened_embedding, p=2, dim=1)

    closest_words = {}
    found_words = set()  # To track words already included in the output
    
    # For each word in key, find the closest embeddings
    for i, k_idx in enumerate(key_indices):
        key_embedding = flattened_embedding[k_idx]  # Get the embedding of the key word
        
        # Calculate cosine similarities with all other embeddings
        similarities = torch.matmul(flattened_embedding, key_embedding)
        
        # Get the top N closest words (excluding the key word itself)
        _, top_indices = torch.topk(similarities, len(words))  # Get indices in descending similarity order
        
        closest_for_key = []
        
        for idx in top_indices:
            if idx != k_idx and words[idx] not in found_words:
                closest_for_key.append(words[idx])
                found_words.add(words[idx])  # Mark the word as used
            if len(closest_for_key) == top_n:  # Stop when we reach the desired number of words
                break

        closest_words[words[k_idx]] = closest_for_key

    return closest_words

# Example usage
words = [...]  # Your list of 5676 words
flattened_embedding = torch.tensor([...])  # Your tensor of embeddings
key = ['word1', 'word2', 'word3']  # Your list of 3 key words

# Get the closest words to each word in the key list
closest_words = find_closest_words(words, flattened_embedding, key)

# Output the result
for k, v in closest_words.items():
    print(f"Closest words to '{k}': {v}")