import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Function to calculate cosine similarity using torch.matmul
def cosine_similarity_torch(embedding_matrix):
    # Normalize the embedding matrix
    norms = torch.norm(embedding_matrix, dim=1, keepdim=True)
    normalized_embeddings = embedding_matrix / norms

    # Compute cosine similarity using torch.matmul
    cosine_sim_matrix = torch.matmul(normalized_embeddings, normalized_embeddings.T)
    return cosine_sim_matrix

# Function to find closest words using cosine similarity
def find_closest_words_torch(words, flattened_embedding, key, top_n=5):
    closest_words = {}

    # Compute cosine similarity matrix using torch.matmul
    cosine_sim_matrix = cosine_similarity_torch(flattened_embedding)

    for k in key:
        key_idx = words.index(k)  # Get index of key word
        similarities = cosine_sim_matrix[key_idx]  # Cosine similarities for the key word

        # Get indices of top_n most similar words (excluding the key word itself)
        _, closest_indices = torch.topk(similarities, top_n + 1)  # Get top_n + 1 to exclude itself

        # Filter out the key word from the closest indices
        closest_indices = closest_indices[closest_indices != key_idx][:top_n]  # Exclude the key word index

        # Store the closest words in the dictionary
        closest_words[k] = [words[i] for i in closest_indices]
    
    return closest_words

# Function to visualize the closest words using cosine similarity
def visualize_with_cosine_similarity_torch(words, flattened_embedding, key, closest_words):
    # Create a list of embeddings to visualize (key words + their closest words)
    embeddings_to_plot = []
    labels = []

    for k in key:
        embeddings_to_plot.append(flattened_embedding[words.index(k)].numpy())  # Key word embedding
        labels.append(k)
        
        for closest_word in closest_words[k]:
            embeddings_to_plot.append(flattened_embedding[words.index(closest_word)].numpy())  # Closest word embedding
            labels.append(closest_word)
    
    embeddings_to_plot = np.array(embeddings_to_plot)

    # Compute the cosine similarity matrix using torch.matmul
    cosine_sim_matrix = cosine_similarity_torch(torch.tensor(embeddings_to_plot))

    # Compute cosine distance (1 - similarity)
    cosine_dist_matrix = 1 - cosine_sim_matrix.numpy()

    # Use t-SNE with the precomputed distance matrix
    tsne = TSNE(n_components=2, metric="precomputed", random_state=42)
    reduced_embeddings = tsne.fit_transform(cosine_dist_matrix)

    # Plot the result
    plt.figure(figsize=(10, 10))
    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c='blue', marker='o')

    # Highlight the key words in red
    for i, label in enumerate(labels):
        if label in key:
            plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], c='red', marker='x', s=100)
            plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], label, fontsize=12, color='red')
        else:
            plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], label, fontsize=9, color='blue')

    plt.title("t-SNE Visualization with Cosine Similarity (Torch Matmul)")
    plt.show()

# Example usage
words = ['finance', 'investment', 'bank', 'loan', 'money', 'economy', 'capital', 'market', 'debt', 'credit', 'growth']  # Your list of words
flattened_embedding = torch.randn(len(words), 600)  # Random embeddings (replace with actual embeddings)
key = ['finance', 'loan', 'market']  # Your list of 3 key words

# Find the closest words for each key word
closest_words = find_closest_words_torch(words, flattened_embedding, key, top_n=5)

# Visualize the closest words using cosine similarity
visualize_with_cosine_similarity_torch(words, flattened_embedding, key, closest_words)