\documentclass{article}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\captionsetup[table]{skip=8pt}

\begin{document}

\begin{table}[h!]
\centering
\small
\caption{Technical Feature Comparison: BART-Large vs BART-Base vs Pegasus}
\begin{tabular}{|p{5.6cm}|p{2.4cm}|p{2.4cm}|p{2.4cm}|}
\hline
\textbf{Feature} & \textbf{BART-Large} & \textbf{BART-Base} & \textbf{Pegasus} \\
\hline
Model Type (Encoder-decoder Transformer) & Yes & Yes & Yes \\
\hline
Total Parameters (Trainable weights in millions) & 406M & 139M & 568M \\
\hline
Encoder Layers (Number of encoder transformer blocks) & 12 & 6 & 16 \\
\hline
Decoder Layers (Number of decoder transformer blocks) & 12 & 6 & 16 \\
\hline
Hidden Size (Dimension of hidden states and embeddings) & 1024 & 768 & 1024 \\
\hline
Feedforward Network Size (Intermediate layer size) & 4096 & 3072 & 4096 \\
\hline
Attention Heads (Number of parallel attention heads) & 16 & 12 & 16 \\
\hline
Max Position Embeddings (Token limit per input) & 1024 & 1024 & 1024 \\
\hline
Pretraining Objective (Learning strategy) & Denoising autoencoder & Denoising autoencoder & Gap sentence generation (GSG) \\
\hline
Tokenizer (Subword tokenizer type) & BPE (fairseq) & BPE (fairseq) & SentencePiece \\
\hline
Pretraining Corpus (Data sources used) & BookCorpus, CC-News, Wikipedia, etc. & Same as BART-Large & C4, HugeNews, Wikipedia, etc. \\
\hline
Pretraining Tokens (Estimated total tokens seen) & \textasciitilde160B & \textasciitilde160B & \textasciitilde500B \\
\hline
Training Framework (Original implementation) & fairseq (PyTorch) & fairseq (PyTorch) & TensorFlow + fairseq \\
\hline
Finetuning Applications (Downstream task adaptability) & Summarization, QA, Translation & Same & Summarization-focused \\
\hline
Language Coverage (Languages supported) & English only & English only & English only \\
\hline
Typical Use Case (Best suited applications) & General-purpose NLP & General-purpose NLP & Abstractive summarization \\
\hline
\end{tabular}
\end{table}

\end{document}