Perfect ‚Äî let‚Äôs break this down into a clear, working example for fine-tuning your local T5 model as a summarizer, including ROUGE metric, Trainer, and hyperparameter tuning.

I‚Äôll assume:
	‚Ä¢	You‚Äôre using Hugging Face Transformers
	‚Ä¢	You have your T5 checkpoint saved locally
	‚Ä¢	Your data is text + summary pairs (e.g., in a CSV or JSONL)

‚∏ª

‚úÖ 1Ô∏è‚É£ Setup

pip install transformers datasets evaluate


‚∏ª

‚úÖ 2Ô∏è‚É£ Load libraries

from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq
from datasets import load_dataset, Dataset
import evaluate
import numpy as np


‚∏ª

‚úÖ 3Ô∏è‚É£ Load your local model & tokenizer

model_path = "./your_local_t5"  # update this!
model = T5ForConditionalGeneration.from_pretrained(model_path)
tokenizer = T5Tokenizer.from_pretrained(model_path)


‚∏ª

‚úÖ 4Ô∏è‚É£ Load and prepare dataset

Example: suppose you have a CSV with text and summary columns.

from datasets import load_dataset

# Replace with your dataset path and format
dataset = load_dataset('csv', data_files={'train': 'train.csv', 'validation': 'val.csv'})

# Add "summarize: " prefix for T5
def preprocess_function(examples):
    inputs = ["summarize: " + doc for doc in examples["text"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True)

    # Tokenize targets
    labels = tokenizer(examples["summary"], max_length=128, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset.map(preprocess_function, batched=True)


‚∏ª

‚úÖ 5Ô∏è‚É£ Data collator

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)


‚∏ª

‚úÖ 6Ô∏è‚É£ ROUGE metric

rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {k: round(v * 100, 4) for k, v in result.items()}
    return result


‚∏ª

‚úÖ 7Ô∏è‚É£ Training arguments

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-4,   # change for tuning!
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,  # if you have GPU with mixed precision
)


‚∏ª

‚úÖ 8Ô∏è‚É£ Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)


‚∏ª

‚úÖ 9Ô∏è‚É£ Start training

trainer.train()


‚∏ª

‚úÖ 10Ô∏è‚É£ Hyperparameter tuning

You can wrap training in a loop for manual grid search:

for lr in [5e-5, 3e-4, 1e-4]:
    for batch_size in [4, 8]:
        training_args = TrainingArguments(
            output_dir=f"./results_lr{lr}_bs{batch_size}",
            evaluation_strategy="epoch",
            learning_rate=lr,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            weight_decay=0.01,
            num_train_epochs=3,
            predict_with_generate=True,
            fp16=True,
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_datasets["train"],
            eval_dataset=tokenized_datasets["validation"],
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics,
        )
        trainer.train()


‚∏ª

‚úÖ üìå Notes
	‚Ä¢	For large models: lower batch size, use gradient accumulation if needed.
	‚Ä¢	If VRAM runs out: clear GPU as we discussed earlier.
	‚Ä¢	You can plug this into Optuna for automated tuning if you want.

‚∏ª

If you want, I can generate a complete Python script for this ‚Äî or adapt it for your exact data format.
Want me to? Just say yes and share:
	‚Ä¢	Your data format (CSV/JSONL/other)
	‚Ä¢	Local T5 model folder name