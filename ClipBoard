\documentclass{article}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\captionsetup[table]{skip=8pt}

\begin{document}

\begin{table}[h!]
\centering
\small
\caption{Technical Feature Comparison: BART-Large vs BART-Base vs Pegasus}
\begin{tabular}{@{}p{5.6cm}p{2.4cm}p{2.4cm}p{2.4cm}@{}}
\toprule
\textbf{Feature} & \textbf{BART-Large} & \textbf{BART-Base} & \textbf{Pegasus} \\
\midrule
Model Type (Encoder-decoder Transformer) & Yes & Yes & Yes \\
Total Parameters (Trainable weights in millions) & 406M & 139M & 568M \\
Encoder Layers (Number of encoder transformer blocks) & 12 & 6 & 16 \\
Decoder Layers (Number of decoder transformer blocks) & 12 & 6 & 16 \\
Hidden Size (Dimension of hidden states and embeddings) & 1024 & 768 & 1024 \\
Feedforward Network Size (Intermediate layer size) & 4096 & 3072 & 4096 \\
Attention Heads (Number of parallel attention heads) & 16 & 12 & 16 \\
Max Position Embeddings (Token limit per input) & 1024 & 1024 & 1024 \\
Pretraining Objective (Learning strategy) & Denoising autoencoder & Denoising autoencoder & Gap sentence generation (GSG) \\
Tokenizer (Subword tokenizer type) & BPE (fairseq) & BPE (fairseq) & SentencePiece \\
Pretraining Corpus (Data sources used) & BookCorpus, CC-News, Wikipedia, etc. & Same as BART-Large & C4, HugeNews, Wikipedia, etc. \\
Pretraining Tokens (Estimated total tokens seen) & ~160B & ~160B & ~500B \\
Training Framework (Original implementation) & fairseq (PyTorch) & fairseq (PyTorch) & TensorFlow + fairseq \\
Finetuning Applications (Downstream task adaptability) & Summarization, QA, Translation & Same & Summarization-focused \\
Language Coverage (Languages supported) & English only & English only & English only \\
Typical Use Case (Best suited applications) & General-purpose NLP & General-purpose NLP & Abstractive summarization \\
\bottomrule
\end{tabular}
\end{table}

\end{document}