import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification

# Constants
MODEL_PATH = 'best_model_card_decline.pt'
EXCEL_FILE = 'new_intents.xlsx'
SHEET_NAME = 'x'
MAX_LEN = 128

# 1. Load Excel Data
df_new = pd.read_excel(EXCEL_FILE, sheet_name=SHEET_NAME)
assert 'text' in df_new.columns, "Input Excel must have a 'text' column."
texts = df_new['text'].astype(str).fillna('').tolist()

# 2. Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# You need to reinitialize the model with correct num_labels
# Load label encoder (from training time)
import pickle
with open('label_encoder.pkl', 'rb') as f:
    le = pickle.load(f)

num_labels = len(le.classes_)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)
model.load_state_dict(torch.load(MODEL_PATH))
model.eval()
model.cuda()

# 3. Predict without batching
predictions = []
probs = []

for text in texts:
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=MAX_LEN)
    inputs = {k: v.cuda() for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        pred_id = torch.argmax(logits, dim=1).item()
        prob = torch.softmax(logits, dim=1)[0][pred_id].item()
        predictions.append(le.inverse_transform([pred_id])[0])
        probs.append(prob)

# 4. Add predictions to DataFrame
df_new['predicted_intent'] = predictions
df_new['confidence'] = probs

# 5. Save to new file
df_new.to_excel('scored_intents.xlsx', index=False)

print("âœ… Predictions saved to 'scored_intents.xlsx'")