Positional encoding adds information about token positions to embeddings, enabling transformers to capture word order.
