Hereâ€™s a clean and concise slide content summarizing Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) â€” the two pretraining objectives of BERT:

â¸»

ðŸ§  BERT Pretraining Tasks

1. Masked Language Modeling (MLM)

Goal: Predict randomly masked words in a sentence

	â€¢	15% of input tokens are masked
	â€¢	Model learns contextual understanding of words
	â€¢	Example:
	â€¢	Input: The cat sat on the [MASK].
	â€¢	Output: mat

â¸»

2. Next Sentence Prediction (NSP)

Goal: Predict whether one sentence follows another

	â€¢	50%: Second sentence is the true next
	â€¢	50%: Random sentence from corpus
	â€¢	Helps model understand sentence relationships
	â€¢	Example:
	â€¢	Sentence A: The man went to the store.
	â€¢	Sentence B: He bought some milk. â†’ IsNext
	â€¢	Sentence B: The sky is blue. â†’ NotNext

â¸»

ðŸ“Œ Purpose: Together, MLM + NSP enable BERT to learn both deep word context and inter-sentence coherence â€” key for downstream tasks like QA, NLI, etc.

â¸»

Let me know if you want a visual diagram or LaTeX version for the slide.