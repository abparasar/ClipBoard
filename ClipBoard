import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score

# Assuming you have a DataFrame score_df with columns true_intents, predicted_intent, and prob
# score_df = pd.DataFrame({'true_intents': [...], 'predicted_intent': [...], 'prob': [...]})

# Initialize lists to store metrics for different thresholds
thresholds = np.linspace(0, 1, 100)  # Thresholds from 0 to 1
macro_precision_list = []
macro_recall_list = []
macro_f1_list = []
macro_f05_list = []
micro_precision_list = []
micro_recall_list = []
micro_f1_list = []
micro_f05_list = []

# Iterate over different thresholds
for threshold in thresholds:
    # Classify as positive if the probability is greater than or equal to the threshold
    predicted_at_threshold = np.where(score_df['prob'] >= threshold, score_df['predicted_intent'], 'negative_class')

    # Calculate macro and micro precision, recall, F1, and F0.5
    macro_precision = precision_score(score_df['true_intents'], predicted_at_threshold, average='macro', zero_division=0)
    macro_recall = recall_score(score_df['true_intents'], predicted_at_threshold, average='macro', zero_division=0)
    macro_f1 = f1_score(score_df['true_intents'], predicted_at_threshold, average='macro', zero_division=0)
    macro_f05 = f1_score(score_df['true_intents'], predicted_at_threshold, average='macro', beta=0.5, zero_division=0)

    micro_precision = precision_score(score_df['true_intents'], predicted_at_threshold, average='micro', zero_division=0)
    micro_recall = recall_score(score_df['true_intents'], predicted_at_threshold, average='micro', zero_division=0)
    micro_f1 = f1_score(score_df['true_intents'], predicted_at_threshold, average='micro', zero_division=0)
    micro_f05 = f1_score(score_df['true_intents'], predicted_at_threshold, average='micro', beta=0.5, zero_division=0)

    # Store the results for each threshold
    macro_precision_list.append(macro_precision)
    macro_recall_list.append(macro_recall)
    macro_f1_list.append(macro_f1)
    macro_f05_list.append(macro_f05)
    micro_precision_list.append(micro_precision)
    micro_recall_list.append(micro_recall)
    micro_f1_list.append(micro_f1)
    micro_f05_list.append(micro_f05)

# Plot the results
plt.figure(figsize=(12, 8))

plt.plot(thresholds, macro_precision_list, label='Macro Precision')
plt.plot(thresholds, macro_recall_list, label='Macro Recall')
plt.plot(thresholds, macro_f1_list, label='Macro F1')
plt.plot(thresholds, macro_f05_list, label='Macro F0.5')

plt.plot(thresholds, micro_precision_list, label='Micro Precision', linestyle='--')
plt.plot(thresholds, micro_recall_list, label='Micro Recall', linestyle='--')
plt.plot(thresholds, micro_f1_list, label='Micro F1', linestyle='--')
plt.plot(thresholds, micro_f05_list, label='Micro F0.5', linestyle='--')

# Add labels and title
plt.xlabel('Probability Threshold')
plt.ylabel('Metrics')
plt.title('Classification Metrics vs Probability Threshold')
plt.legend(loc='best')

# Show the plot
plt.grid(True)
plt.show()