🔥 Perfect — let’s level up your notebook-friendly random search to run FAST on 4 GPUs — the right way.
Here’s exactly how you do it step by step, with real multi-GPU DDP using torchrun (the official PyTorch launcher for Trainer).

⸻

✅ Key truth

Real multi-GPU with 🤗 Trainer means:
	•	Each GPU = its own Python process.
	•	Those processes share gradients via PyTorch DDP.
	•	This cannot run inside one .ipynb cell because notebooks run in a single process.

👉 So: your code must go in a .py file, then launch with:

torchrun --nproc_per_node=4 train.py


⸻

⚡️ Here’s your clean multi-GPU train.py

👇 Copy–paste this as train.py:

# train.py

import os
import torch
import pandas as pd
from sklearn.model_selection import train_test_split, ParameterSampler
from datasets import Dataset
from transformers import (
    PegasusTokenizer,
    PegasusForConditionalGeneration,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments
)
import evaluate
import numpy as np

# ✅ Required for DDP
local_rank = int(os.environ.get("LOCAL_RANK", 0))
device = torch.device("cuda", local_rank) if torch.cuda.is_available() else torch.device("cpu")

print(f"Rank {local_rank} using {device}")

# ✅ Load dataset
df = pd.read_csv("/projectspace/zkfq4hl/data/titlesum.csv").head(1000)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-large")
model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-large").to(device)

# ✅ Tokenize
def preprocess(examples):
    inputs = examples["dialogue"]
    model_inputs = tokenizer(
        inputs, max_length=1024, truncation=True, padding="max_length"
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["one_liner"],
            max_length=1024, truncation=True, padding="max_length"
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_ds = Dataset.from_pandas(train_df).map(preprocess, batched=True)
val_ds = Dataset.from_pandas(val_df).map(preprocess, batched=True)

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if isinstance(predictions, tuple):
        predictions = predictions[0]
    predictions = np.where(predictions >= 0, predictions, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return {k: round(v, 4) for k, v in rouge.compute(predictions=decoded_preds, references=decoded_labels).items()}

# ✅ Random search
param_dist = {
    "learning_rate": [1e-5, 3e-5, 5e-5],
    "per_device_train_batch_size": [2, 4],
    "num_train_epochs": [2, 3],
    "weight_decay": [0.0, 0.01],
    "warmup_steps": [0, 500],
}

param_sampler = ParameterSampler(param_dist, n_iter=5, random_state=42)

best_metric = -1
best_params = None

for i, params in enumerate(param_sampler):
    run_name = f"run_{i+1}_lr{params['learning_rate']}_bs{params['per_device_train_batch_size']}_ep{params['num_train_epochs']}_wd{params['weight_decay']}_wu{params['warmup_steps']}"
    output_dir = f"./pegasus_random_runs/{run_name}"

    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=params["learning_rate"],
        per_device_train_batch_size=params["per_device_train_batch_size"],
        per_device_eval_batch_size=params["per_device_train_batch_size"],
        num_train_epochs=params["num_train_epochs"],
        weight_decay=params["weight_decay"],
        warmup_steps=params["warmup_steps"],
        predict_with_generate=True,
        generation_max_length=1024,
        fp16=True,
        save_total_limit=1,
        logging_dir=f"{output_dir}/logs",
        logging_steps=50,
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="eval_rougeL",
        greater_is_better=True
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    if local_rank == 0:
        print(f"\n🚀 Starting run {i+1}: {run_name}\n{'-'*60}")

    trainer.train()
    metrics = trainer.evaluate()
    rougeL = metrics.get("eval_rougeL", 0)

    if local_rank == 0:
        print(f"Run {run_name} → ROUGE-L: {rougeL:.4f}")

    if rougeL > best_metric:
        best_metric = rougeL
        best_params = params

if local_rank == 0:
    print("\n✅ Finished all runs.")
    print(f"Best ROUGE-L: {best_metric:.4f}")
    print(f"Best Params: {best_params}")


⸻

✅ How to launch it

torchrun --nproc_per_node=4 train.py

This spawns 4 parallel Python processes, each on its own GPU:
	•	local_rank=0 → GPU 0
	•	local_rank=1 → GPU 1
	•	… and so on.

The 🤗 Trainer automatically syncs gradients across them — no extra code needed!

⸻

✅ Things to remember

✅ Per-device batch → each GPU sees per_device_train_batch_size.
So total batch = per_device_train_batch_size × num_GPUs.

✅ Logs & checkpoints only come from rank 0 — so you don’t get duplicate output.

✅ If you want fp16 → set fp16=True.

⸻

✅ How to run from notebook

You can’t do this directly inside .ipynb.
But you can launch it from a notebook cell with !:

!torchrun --nproc_per_node=4 train.py


⸻

✅ This is 100% the standard best practice for real multi-GPU Pegasus fine-tuning.

When you’re ready for:
	•	Cluster/Slurm job
	•	W&B sweep logging
	•	Better reproducibility

Just say: “Show me that next!” 🚀


✅ Good call — by default, 🤗 Trainer saves the best checkpoint for each run, because you used:

load_best_model_at_end=True,
metric_for_best_model="eval_rougeL",
greater_is_better=True

But that only keeps the best checkpoint per run (e.g. checkpoint-xxx).
If you want to automatically copy the final best model to a named folder at the end of your entire random search, just add this 👇 after your loop:

⸻

✅ Add to your train.py

from transformers import AutoModelForSeq2SeqLM

if local_rank == 0:
    print("\n✅ Finished all runs.")
    print(f"Best ROUGE-L: {best_metric:.4f}")
    print(f"Best Params: {best_params}")

    # 💾 Save final best model under a clear name
    final_output_dir = "./pegasus_random_runs/best_model_final"

    # Re-train with best params (or just reload last checkpoint)
    best_run_name = f"best_run_lr{best_params['learning_rate']}_bs{best_params['per_device_train_batch_size']}_ep{best_params['num_train_epochs']}_wd{best_params['weight_decay']}_wu{best_params['warmup_steps']}"
    best_output_dir = f"./pegasus_random_runs/{best_run_name}"

    print(f"\n✅ Saving final best model from: {best_output_dir}")

    # Reload best model from output_dir
    best_model = PegasusForConditionalGeneration.from_pretrained(best_output_dir)
    best_model.save_pretrained(final_output_dir)
    tokenizer.save_pretrained(final_output_dir)

    print(f"\n🚀 Best model saved to: {final_output_dir}")


⸻

🔑 What this does

✅ Keeps the final best model in:

./pegasus_random_runs/best_model_final

✅ Uses the best run’s folder you trained in → reloads it → saves it cleanly.

✅ You can then from_pretrained() from that folder in future runs:

model = PegasusForConditionalGeneration.from_pretrained("./pegasus_random_runs/best_model_final")


⸻

⚡️ That’s it!

This makes your random search fully self-contained:
	•	Each trial → best checkpoint saved.
	•	Final best → cleanly exported to one folder.

⸻

If you’d like, I can show you:
	•	How to push this folder directly to 🤗 Hub.
	•	How to load and generate with your final model.
	•	Or how to log all runs nicely with wandb.

Just say “Show me next!” 🚀