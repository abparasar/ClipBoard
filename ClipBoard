Self-attention is calculated for every word with respect to all other words, including itself. It helps the model figure out that “he” refers to “Ron” by checking connections between words. This way, the sentence meaning is understood better.